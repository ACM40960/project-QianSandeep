{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch daily S&P 500 data\n",
    "sp500_data = yf.download('^GSPC', start='2000-01-01', end='2024-01-01')\n",
    "\n",
    "# Calculate additional features for S&P 500 data\n",
    "sp500_data['daily_return'] = sp500_data['Close'].pct_change() #might need\n",
    "#sp500_data['volatility'] = sp500_data['daily_return'].rolling(window=20).std()\n",
    "\n",
    "###sp500_data['ma_200'] = sp500_data['Close'].rolling(window=200).mean() # for long term\n",
    "\n",
    "\n",
    "# Calculate the 200-day moving return\n",
    "sp500_data['200_day_return'] = sp500_data['Close'].pct_change(periods=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>200_day_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-10-17</th>\n",
       "      <td>1374.619995</td>\n",
       "      <td>1380.989990</td>\n",
       "      <td>1342.339966</td>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1161500000</td>\n",
       "      <td>-0.017932</td>\n",
       "      <td>-0.072326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-18</th>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1356.650024</td>\n",
       "      <td>1305.790039</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1441700000</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>-0.040938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-19</th>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1389.930054</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1297900000</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>-0.009521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-20</th>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1408.469971</td>\n",
       "      <td>1382.189941</td>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1177400000</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>-0.004646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-23</th>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1406.959961</td>\n",
       "      <td>1387.750000</td>\n",
       "      <td>1395.780029</td>\n",
       "      <td>1395.780029</td>\n",
       "      <td>1046800000</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>-0.031697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-24</th>\n",
       "      <td>1395.780029</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>1388.130005</td>\n",
       "      <td>1398.130005</td>\n",
       "      <td>1398.130005</td>\n",
       "      <td>1158600000</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>-0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-25</th>\n",
       "      <td>1398.130005</td>\n",
       "      <td>1398.130005</td>\n",
       "      <td>1362.209961</td>\n",
       "      <td>1364.900024</td>\n",
       "      <td>1364.900024</td>\n",
       "      <td>1315600000</td>\n",
       "      <td>-0.023767</td>\n",
       "      <td>-0.051204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-26</th>\n",
       "      <td>1364.900024</td>\n",
       "      <td>1372.719971</td>\n",
       "      <td>1337.810059</td>\n",
       "      <td>1364.439941</td>\n",
       "      <td>1364.439941</td>\n",
       "      <td>1303800000</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.047345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-27</th>\n",
       "      <td>1364.439941</td>\n",
       "      <td>1384.569946</td>\n",
       "      <td>1364.130005</td>\n",
       "      <td>1379.579956</td>\n",
       "      <td>1379.579956</td>\n",
       "      <td>1086300000</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>-0.048356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-30</th>\n",
       "      <td>1379.579956</td>\n",
       "      <td>1406.359985</td>\n",
       "      <td>1376.859985</td>\n",
       "      <td>1398.660034</td>\n",
       "      <td>1398.660034</td>\n",
       "      <td>1186500000</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>-0.045381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-31</th>\n",
       "      <td>1398.660034</td>\n",
       "      <td>1432.219971</td>\n",
       "      <td>1398.660034</td>\n",
       "      <td>1429.400024</td>\n",
       "      <td>1429.400024</td>\n",
       "      <td>1366400000</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>-0.017689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-01</th>\n",
       "      <td>1429.400024</td>\n",
       "      <td>1429.599976</td>\n",
       "      <td>1410.449951</td>\n",
       "      <td>1421.219971</td>\n",
       "      <td>1421.219971</td>\n",
       "      <td>1206800000</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>-0.023820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-02</th>\n",
       "      <td>1421.219971</td>\n",
       "      <td>1433.400024</td>\n",
       "      <td>1421.219971</td>\n",
       "      <td>1428.319946</td>\n",
       "      <td>1428.319946</td>\n",
       "      <td>1167700000</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>-0.011933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-03</th>\n",
       "      <td>1428.319946</td>\n",
       "      <td>1433.209961</td>\n",
       "      <td>1420.920044</td>\n",
       "      <td>1426.689941</td>\n",
       "      <td>1426.689941</td>\n",
       "      <td>997700000</td>\n",
       "      <td>-0.001141</td>\n",
       "      <td>-0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-06</th>\n",
       "      <td>1428.760010</td>\n",
       "      <td>1438.459961</td>\n",
       "      <td>1427.719971</td>\n",
       "      <td>1432.189941</td>\n",
       "      <td>1432.189941</td>\n",
       "      <td>930900000</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.021876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-07</th>\n",
       "      <td>1432.189941</td>\n",
       "      <td>1436.219971</td>\n",
       "      <td>1423.260010</td>\n",
       "      <td>1431.869995</td>\n",
       "      <td>1431.869995</td>\n",
       "      <td>880900000</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>0.015489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-08</th>\n",
       "      <td>1431.869995</td>\n",
       "      <td>1437.280029</td>\n",
       "      <td>1408.780029</td>\n",
       "      <td>1409.280029</td>\n",
       "      <td>1409.280029</td>\n",
       "      <td>909300000</td>\n",
       "      <td>-0.015777</td>\n",
       "      <td>0.003696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-09</th>\n",
       "      <td>1409.280029</td>\n",
       "      <td>1409.280029</td>\n",
       "      <td>1369.680054</td>\n",
       "      <td>1400.140015</td>\n",
       "      <td>1400.140015</td>\n",
       "      <td>1111000000</td>\n",
       "      <td>-0.006486</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-10</th>\n",
       "      <td>1400.140015</td>\n",
       "      <td>1400.140015</td>\n",
       "      <td>1365.969971</td>\n",
       "      <td>1365.979980</td>\n",
       "      <td>1365.979980</td>\n",
       "      <td>962500000</td>\n",
       "      <td>-0.024398</td>\n",
       "      <td>0.004279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-13</th>\n",
       "      <td>1365.979980</td>\n",
       "      <td>1365.979980</td>\n",
       "      <td>1328.619995</td>\n",
       "      <td>1351.260010</td>\n",
       "      <td>1351.260010</td>\n",
       "      <td>1129300000</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>-0.030980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2000-10-17  1374.619995  1380.989990  1342.339966  1349.969971  1349.969971   \n",
       "2000-10-18  1349.969971  1356.650024  1305.790039  1342.130005  1342.130005   \n",
       "2000-10-19  1342.130005  1389.930054  1342.130005  1388.760010  1388.760010   \n",
       "2000-10-20  1388.760010  1408.469971  1382.189941  1396.930054  1396.930054   \n",
       "2000-10-23  1396.930054  1406.959961  1387.750000  1395.780029  1395.780029   \n",
       "2000-10-24  1395.780029  1415.640015  1388.130005  1398.130005  1398.130005   \n",
       "2000-10-25  1398.130005  1398.130005  1362.209961  1364.900024  1364.900024   \n",
       "2000-10-26  1364.900024  1372.719971  1337.810059  1364.439941  1364.439941   \n",
       "2000-10-27  1364.439941  1384.569946  1364.130005  1379.579956  1379.579956   \n",
       "2000-10-30  1379.579956  1406.359985  1376.859985  1398.660034  1398.660034   \n",
       "2000-10-31  1398.660034  1432.219971  1398.660034  1429.400024  1429.400024   \n",
       "2000-11-01  1429.400024  1429.599976  1410.449951  1421.219971  1421.219971   \n",
       "2000-11-02  1421.219971  1433.400024  1421.219971  1428.319946  1428.319946   \n",
       "2000-11-03  1428.319946  1433.209961  1420.920044  1426.689941  1426.689941   \n",
       "2000-11-06  1428.760010  1438.459961  1427.719971  1432.189941  1432.189941   \n",
       "2000-11-07  1432.189941  1436.219971  1423.260010  1431.869995  1431.869995   \n",
       "2000-11-08  1431.869995  1437.280029  1408.780029  1409.280029  1409.280029   \n",
       "2000-11-09  1409.280029  1409.280029  1369.680054  1400.140015  1400.140015   \n",
       "2000-11-10  1400.140015  1400.140015  1365.969971  1365.979980  1365.979980   \n",
       "2000-11-13  1365.979980  1365.979980  1328.619995  1351.260010  1351.260010   \n",
       "\n",
       "                Volume  daily_return  200_day_return  \n",
       "Date                                                  \n",
       "2000-10-17  1161500000     -0.017932       -0.072326  \n",
       "2000-10-18  1441700000     -0.005808       -0.040938  \n",
       "2000-10-19  1297900000      0.034743       -0.009521  \n",
       "2000-10-20  1177400000      0.005883       -0.004646  \n",
       "2000-10-23  1046800000     -0.000823       -0.031697  \n",
       "2000-10-24  1158600000      0.001684       -0.040800  \n",
       "2000-10-25  1315600000     -0.023767       -0.051204  \n",
       "2000-10-26  1303800000     -0.000337       -0.047345  \n",
       "2000-10-27  1086300000      0.011096       -0.048356  \n",
       "2000-10-30  1186500000      0.013830       -0.045381  \n",
       "2000-10-31  1366400000      0.021978       -0.017689  \n",
       "2000-11-01  1206800000     -0.005723       -0.023820  \n",
       "2000-11-02  1167700000      0.004996       -0.011933  \n",
       "2000-11-03   997700000     -0.001141       -0.010178  \n",
       "2000-11-06   930900000      0.003855        0.021876  \n",
       "2000-11-07   880900000     -0.000223        0.015489  \n",
       "2000-11-08   909300000     -0.015777        0.003696  \n",
       "2000-11-09  1111000000     -0.006486        0.001130  \n",
       "2000-11-10   962500000     -0.024398        0.004279  \n",
       "2000-11-13  1129300000     -0.010776       -0.030980  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500 = sp500_data.dropna()\n",
    "sp500.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as pdr\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch economic indicators from FRED\n",
    "gdp = pdr.get_data_fred('GDP', start='2000-01-01', end='2024-01-01') #quaterly\n",
    "\n",
    "inflation = pdr.get_data_fred('CPIAUCSL', start='2000-01-01', end='2024-01-01')  # CPI for inflation\n",
    "unemployment = pdr.get_data_fred('UNRATE', start='2000-01-01', end='2024-01-01')\n",
    "interest_rate = pdr.get_data_fred('FEDFUNDS', start='2000-01-01', end='2024-01-01')\n",
    "\n",
    "# Rename columns for clarity\n",
    "gdp.rename(columns={'GDP': 'GDP'}, inplace=True)\n",
    "inflation.rename(columns={'CPIAUCSL': 'Inflation'}, inplace=True)\n",
    "unemployment.rename(columns={'UNRATE': 'Unemployment'}, inplace=True)\n",
    "interest_rate.rename(columns={'FEDFUNDS': 'Interest_Rate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>21706.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>21706.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>21706.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>21706.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>19913.143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GDP\n",
       "DATE                 \n",
       "2020-03-28  21706.513\n",
       "2020-03-29  21706.513\n",
       "2020-03-30  21706.513\n",
       "2020-03-31  21706.513\n",
       "2020-04-01  19913.143"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP = gdp.resample('D').ffill()\n",
    "GDP.loc['2020-03-28':'2020-04-01'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inflation = inflation.resample('D').ffill()\n",
    "Unemployment = unemployment.resample('D').ffill()\n",
    "Interest_rate = interest_rate.resample('D').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>200_day_return</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Interest_Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-10-17</th>\n",
       "      <td>1374.619995</td>\n",
       "      <td>1380.989990</td>\n",
       "      <td>1342.339966</td>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1.161500e+09</td>\n",
       "      <td>-0.017932</td>\n",
       "      <td>-0.072326</td>\n",
       "      <td>173.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-18</th>\n",
       "      <td>1349.969971</td>\n",
       "      <td>1356.650024</td>\n",
       "      <td>1305.790039</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1.441700e+09</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>-0.040938</td>\n",
       "      <td>173.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-19</th>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1389.930054</td>\n",
       "      <td>1342.130005</td>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1.297900e+09</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>173.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-20</th>\n",
       "      <td>1388.760010</td>\n",
       "      <td>1408.469971</td>\n",
       "      <td>1382.189941</td>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1.177400e+09</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>-0.004646</td>\n",
       "      <td>173.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-23</th>\n",
       "      <td>1396.930054</td>\n",
       "      <td>1406.959961</td>\n",
       "      <td>1387.750000</td>\n",
       "      <td>1395.780029</td>\n",
       "      <td>1395.780029</td>\n",
       "      <td>1.046800e+09</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>-0.031697</td>\n",
       "      <td>173.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2000-10-17  1374.619995  1380.989990  1342.339966  1349.969971  1349.969971   \n",
       "2000-10-18  1349.969971  1356.650024  1305.790039  1342.130005  1342.130005   \n",
       "2000-10-19  1342.130005  1389.930054  1342.130005  1388.760010  1388.760010   \n",
       "2000-10-20  1388.760010  1408.469971  1382.189941  1396.930054  1396.930054   \n",
       "2000-10-23  1396.930054  1406.959961  1387.750000  1395.780029  1395.780029   \n",
       "\n",
       "                  Volume  daily_return  200_day_return  Inflation  \\\n",
       "Date                                                                \n",
       "2000-10-17  1.161500e+09     -0.017932       -0.072326      173.9   \n",
       "2000-10-18  1.441700e+09     -0.005808       -0.040938      173.9   \n",
       "2000-10-19  1.297900e+09      0.034743       -0.009521      173.9   \n",
       "2000-10-20  1.177400e+09      0.005883       -0.004646      173.9   \n",
       "2000-10-23  1.046800e+09     -0.000823       -0.031697      173.9   \n",
       "\n",
       "            Unemployment  Interest_Rate  \n",
       "Date                                     \n",
       "2000-10-17           3.9           6.51  \n",
       "2000-10-18           3.9           6.51  \n",
       "2000-10-19           3.9           6.51  \n",
       "2000-10-20           3.9           6.51  \n",
       "2000-10-23           3.9           6.51  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = sp500.join([Inflation, Unemployment, Interest_rate], how='left')\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add tech indicators\n",
    "import pandas_ta as ta \n",
    "merged_data['rsi'] = ta.rsi(merged_data['Close'], length=14)# this is relative strength index\n",
    "merged_data['emaf']= ta.ema(merged_data['Close'], length=20)# thi is exponential moving average\n",
    "merged_data['emas'] = ta.ema(merged_data['Close'], length=50) # this is exponential moving average\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1\n",
    "\n",
    "We use top 5 companies in US.\n",
    "\n",
    "\n",
    "Apple, Microsoft, Amazon, Google, NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2\n",
    "\n",
    "If we include some independent company as our feature, we avoid adding bias but also we are adding a feature that demonstrates the market state.\n",
    "\n",
    "KKR,\n",
    "Dell Technologies,\t\n",
    "Veeva Systems\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1 :final_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>NVDA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19 00:00:00-04:00</th>\n",
       "      <td>0.463640</td>\n",
       "      <td>16.900316</td>\n",
       "      <td>1.9315</td>\n",
       "      <td>2.508132</td>\n",
       "      <td>0.089124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20 00:00:00-04:00</th>\n",
       "      <td>0.464999</td>\n",
       "      <td>16.950171</td>\n",
       "      <td>1.9755</td>\n",
       "      <td>2.707353</td>\n",
       "      <td>0.094398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23 00:00:00-04:00</th>\n",
       "      <td>0.469226</td>\n",
       "      <td>17.025167</td>\n",
       "      <td>1.9725</td>\n",
       "      <td>2.734599</td>\n",
       "      <td>0.096462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24 00:00:00-04:00</th>\n",
       "      <td>0.482361</td>\n",
       "      <td>17.025167</td>\n",
       "      <td>1.9525</td>\n",
       "      <td>2.621365</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25 00:00:00-04:00</th>\n",
       "      <td>0.498968</td>\n",
       "      <td>17.218922</td>\n",
       "      <td>2.0150</td>\n",
       "      <td>2.649612</td>\n",
       "      <td>0.096844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               AAPL       MSFT    AMZN     GOOGL      NVDA\n",
       "Date                                                                      \n",
       "2004-08-19 00:00:00-04:00  0.463640  16.900316  1.9315  2.508132  0.089124\n",
       "2004-08-20 00:00:00-04:00  0.464999  16.950171  1.9755  2.707353  0.094398\n",
       "2004-08-23 00:00:00-04:00  0.469226  17.025167  1.9725  2.734599  0.096462\n",
       "2004-08-24 00:00:00-04:00  0.482361  17.025167  1.9525  2.621365  0.092869\n",
       "2004-08-25 00:00:00-04:00  0.498968  17.218922  2.0150  2.649612  0.096844"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice the Close range for differ stocks \n",
    "# google has data only from 2004-08-19\n",
    "\n",
    "tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'NVDA']\n",
    "\n",
    "# Dictionary to store close price dataframes\n",
    "close_prices = {}\n",
    "\n",
    "# Fetch historical data for each ticker and extract the close prices\n",
    "for ticker in tickers:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    data = stock.history(start='2004-08-19', end='2024-01-01')\n",
    "    close_prices[ticker] = data[['Close']].rename(columns={'Close': ticker})\n",
    "\n",
    "# Combine all close price dataframes into a single dataframe\n",
    "combined_close_prices = pd.concat(close_prices.values(), axis=1)\n",
    "combined_close_prices.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>volatility</th>\n",
       "      <th>200_day_return</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>NVDA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>1095.170044</td>\n",
       "      <td>1095.170044</td>\n",
       "      <td>1086.280029</td>\n",
       "      <td>1091.229980</td>\n",
       "      <td>1091.229980</td>\n",
       "      <td>1.249400e+09</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.038564</td>\n",
       "      <td>189.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.463640</td>\n",
       "      <td>16.900316</td>\n",
       "      <td>1.9315</td>\n",
       "      <td>2.508132</td>\n",
       "      <td>0.089124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>1091.229980</td>\n",
       "      <td>1100.260010</td>\n",
       "      <td>1089.569946</td>\n",
       "      <td>1098.349976</td>\n",
       "      <td>1098.349976</td>\n",
       "      <td>1.199900e+09</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>0.037138</td>\n",
       "      <td>189.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.464999</td>\n",
       "      <td>16.950171</td>\n",
       "      <td>1.9755</td>\n",
       "      <td>2.707353</td>\n",
       "      <td>0.094398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>1098.349976</td>\n",
       "      <td>1101.400024</td>\n",
       "      <td>1094.729980</td>\n",
       "      <td>1095.680054</td>\n",
       "      <td>1095.680054</td>\n",
       "      <td>1.021900e+09</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.040285</td>\n",
       "      <td>189.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.469226</td>\n",
       "      <td>17.025167</td>\n",
       "      <td>1.9725</td>\n",
       "      <td>2.734599</td>\n",
       "      <td>0.096462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>1095.680054</td>\n",
       "      <td>1100.939941</td>\n",
       "      <td>1092.819946</td>\n",
       "      <td>1096.189941</td>\n",
       "      <td>1096.189941</td>\n",
       "      <td>1.092500e+09</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>189.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.482361</td>\n",
       "      <td>17.025167</td>\n",
       "      <td>1.9525</td>\n",
       "      <td>2.621365</td>\n",
       "      <td>0.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>1096.189941</td>\n",
       "      <td>1106.290039</td>\n",
       "      <td>1093.239990</td>\n",
       "      <td>1104.959961</td>\n",
       "      <td>1104.959961</td>\n",
       "      <td>1.192200e+09</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.044336</td>\n",
       "      <td>189.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.498968</td>\n",
       "      <td>17.218922</td>\n",
       "      <td>2.0150</td>\n",
       "      <td>2.649612</td>\n",
       "      <td>0.096844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2004-08-19  1095.170044  1095.170044  1086.280029  1091.229980  1091.229980   \n",
       "2004-08-20  1091.229980  1100.260010  1089.569946  1098.349976  1098.349976   \n",
       "2004-08-23  1098.349976  1101.400024  1094.729980  1095.680054  1095.680054   \n",
       "2004-08-24  1095.680054  1100.939941  1092.819946  1096.189941  1096.189941   \n",
       "2004-08-25  1096.189941  1106.290039  1093.239990  1104.959961  1104.959961   \n",
       "\n",
       "                  Volume  daily_return  volatility  200_day_return  Inflation  \\\n",
       "Date                                                                            \n",
       "2004-08-19  1.249400e+09     -0.003598    0.008767        0.038564      189.2   \n",
       "2004-08-20  1.199900e+09      0.006525    0.008592        0.037138      189.2   \n",
       "2004-08-23  1.021900e+09     -0.002431    0.008600        0.040285      189.2   \n",
       "2004-08-24  1.092500e+09      0.000465    0.008314        0.042194      189.2   \n",
       "2004-08-25  1.192200e+09      0.008000    0.008500        0.044336      189.2   \n",
       "\n",
       "            Unemployment  Interest_Rate      AAPL       MSFT    AMZN  \\\n",
       "Date                                                                   \n",
       "2004-08-19           5.4           1.43  0.463640  16.900316  1.9315   \n",
       "2004-08-20           5.4           1.43  0.464999  16.950171  1.9755   \n",
       "2004-08-23           5.4           1.43  0.469226  17.025167  1.9725   \n",
       "2004-08-24           5.4           1.43  0.482361  17.025167  1.9525   \n",
       "2004-08-25           5.4           1.43  0.498968  17.218922  2.0150   \n",
       "\n",
       "               GOOGL      NVDA  \n",
       "Date                            \n",
       "2004-08-19  2.508132  0.089124  \n",
       "2004-08-20  2.707353  0.094398  \n",
       "2004-08-23  2.734599  0.096462  \n",
       "2004-08-24  2.621365  0.092869  \n",
       "2004-08-25  2.649612  0.096844  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the date index of combined_close_prices to timezone-naive\n",
    "combined_close_prices.index = combined_close_prices.index.tz_localize(None)\n",
    "merged_data = merged_data.loc['2004-08-19':]\n",
    "\n",
    "final_data = merged_data.join(combined_close_prices, how='inner')\n",
    "\n",
    "final_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2 :final_data too \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KKR</th>\n",
       "      <th>DELL</th>\n",
       "      <th>VEEV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-15</th>\n",
       "      <td>5.904121</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-16</th>\n",
       "      <td>5.608915</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>5.666799</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>5.354227</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>5.093751</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 KKR       DELL   VEEV\n",
       "Date                                  \n",
       "2010-07-15  5.904121  11.303946  37.16\n",
       "2010-07-16  5.608915  11.303946  37.16\n",
       "2010-07-19  5.666799  11.303946  37.16\n",
       "2010-07-20  5.354227  11.303946  37.16\n",
       "2010-07-21  5.093751  11.303946  37.16"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KKR has data only from 2010-07-15--should also change it ones before \n",
    "\n",
    "tickers = ['KKR', 'DELL', 'VEEV']\n",
    "# Dictionary to store close price dataframes\n",
    "close_prices = {}\n",
    "\n",
    "# Fetch historical data for each ticker and extract the close prices\n",
    "for ticker in tickers:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    data = stock.history(start='2010-07-15', end='2024-01-01')\n",
    "    close_prices[ticker] = data[['Close']].rename(columns={'Close': ticker})\n",
    "\n",
    "combined_close_prices = combined_close_prices.ffill()\n",
    "\n",
    "combined_close_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>volatility</th>\n",
       "      <th>200_day_return</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>KKR</th>\n",
       "      <th>DELL</th>\n",
       "      <th>VEEV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-15</th>\n",
       "      <td>1094.459961</td>\n",
       "      <td>1098.660034</td>\n",
       "      <td>1080.530029</td>\n",
       "      <td>1096.479980</td>\n",
       "      <td>1096.479980</td>\n",
       "      <td>4.552470e+09</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>0.031515</td>\n",
       "      <td>217.605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.904121</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-16</th>\n",
       "      <td>1093.849976</td>\n",
       "      <td>1093.849976</td>\n",
       "      <td>1063.319946</td>\n",
       "      <td>1064.880005</td>\n",
       "      <td>1064.880005</td>\n",
       "      <td>5.297350e+09</td>\n",
       "      <td>-0.028819</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>217.605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.608915</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-19</th>\n",
       "      <td>1066.849976</td>\n",
       "      <td>1074.699951</td>\n",
       "      <td>1061.109985</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>4.089500e+09</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>217.605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.666799</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-20</th>\n",
       "      <td>1064.530029</td>\n",
       "      <td>1083.939941</td>\n",
       "      <td>1056.880005</td>\n",
       "      <td>1083.479980</td>\n",
       "      <td>1083.479980</td>\n",
       "      <td>4.713280e+09</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>0.052076</td>\n",
       "      <td>217.605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.354227</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-21</th>\n",
       "      <td>1086.670044</td>\n",
       "      <td>1088.959961</td>\n",
       "      <td>1065.250000</td>\n",
       "      <td>1069.589966</td>\n",
       "      <td>1069.589966</td>\n",
       "      <td>4.747180e+09</td>\n",
       "      <td>-0.012820</td>\n",
       "      <td>0.014376</td>\n",
       "      <td>0.043289</td>\n",
       "      <td>217.605</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.093751</td>\n",
       "      <td>11.303946</td>\n",
       "      <td>37.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2010-07-15  1094.459961  1098.660034  1080.530029  1096.479980  1096.479980   \n",
       "2010-07-16  1093.849976  1093.849976  1063.319946  1064.880005  1064.880005   \n",
       "2010-07-19  1066.849976  1074.699951  1061.109985  1071.250000  1071.250000   \n",
       "2010-07-20  1064.530029  1083.939941  1056.880005  1083.479980  1083.479980   \n",
       "2010-07-21  1086.670044  1088.959961  1065.250000  1069.589966  1069.589966   \n",
       "\n",
       "                  Volume  daily_return  volatility  200_day_return  Inflation  \\\n",
       "Date                                                                            \n",
       "2010-07-15  4.552470e+09      0.001196    0.012680        0.031515    217.605   \n",
       "2010-07-16  5.297350e+09     -0.028819    0.014131        0.004026    217.605   \n",
       "2010-07-19  4.089500e+09      0.005982    0.014231        0.013405    217.605   \n",
       "2010-07-20  4.713280e+09      0.011417    0.014533        0.052076    217.605   \n",
       "2010-07-21  4.747180e+09     -0.012820    0.014376        0.043289    217.605   \n",
       "\n",
       "            Unemployment  Interest_Rate       KKR       DELL   VEEV  \n",
       "Date                                                                 \n",
       "2010-07-15           9.4           0.18  5.904121  11.303946  37.16  \n",
       "2010-07-16           9.4           0.18  5.608915  11.303946  37.16  \n",
       "2010-07-19           9.4           0.18  5.666799  11.303946  37.16  \n",
       "2010-07-20           9.4           0.18  5.354227  11.303946  37.16  \n",
       "2010-07-21           9.4           0.18  5.093751  11.303946  37.16  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the date index of combined_close_prices to timezone-naive\n",
    "combined_close_prices.index = combined_close_prices.index.tz_localize(None)\n",
    "merged_data = merged_data.loc['2010-07-15':]\n",
    "\n",
    "final_data = merged_data.join(combined_close_prices, how='inner')\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3388,) (3388, 12)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.45      0.46       308\n",
      "           1       0.56      0.57      0.56       370\n",
      "\n",
      "    accuracy                           0.52       678\n",
      "   macro avg       0.51      0.51      0.51       678\n",
      "weighted avg       0.52      0.52      0.52       678\n",
      "\n",
      "Out-of-Sample Accuracy: 0.5176991150442478\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# y\n",
    "y = np.where(final_data['Close'].shift(-1) > merged_data['Close'], 1, -1)#explain: if the next day's close price is higher than the current day's close price, the model should predict a 1, otherwise, it should predict a -1.\n",
    "# feature\n",
    "X = final_data.drop(columns=['Close', 'daily_return', '200_day_return']).shift(1)\n",
    "\n",
    "\n",
    "y = y[:-1]\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "y = y[-len(X):]\n",
    "\n",
    "# Check the shape of 'y' and 'X'\n",
    "print(y.shape, X.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "# Calculate the accuracy score\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Out-of-Sample Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "###not improved much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.13      0.21       308\n",
      "           1       0.56      0.93      0.70       370\n",
      "\n",
      "    accuracy                           0.57       678\n",
      "   macro avg       0.59      0.53      0.46       678\n",
      "weighted avg       0.59      0.57      0.48       678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Create an instance of SVM\n",
    "model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma=0.1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9934119325120795\n",
      "R2 Score: -0.001789160198496731\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R2 Score: {r2}')\n",
    "# accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianqianmeng/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.9977\n",
      "Epoch 2/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9937\n",
      "Epoch 3/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9932\n",
      "Epoch 4/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9930\n",
      "Epoch 5/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9928\n",
      "Epoch 6/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9926\n",
      "Epoch 7/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9925\n",
      "Epoch 8/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9924\n",
      "Epoch 9/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9923\n",
      "Epoch 10/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9922\n",
      "Epoch 11/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9921\n",
      "Epoch 12/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9921\n",
      "Epoch 13/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9920\n",
      "Epoch 14/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9919\n",
      "Epoch 15/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9919\n",
      "Epoch 16/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9918\n",
      "Epoch 17/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9918\n",
      "Epoch 18/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9918\n",
      "Epoch 19/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9917\n",
      "Epoch 20/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9917\n",
      "Epoch 21/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9916\n",
      "Epoch 22/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9916\n",
      "Epoch 23/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9916\n",
      "Epoch 24/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9916\n",
      "Epoch 25/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9915\n",
      "Epoch 26/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9915\n",
      "Epoch 27/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9915\n",
      "Epoch 28/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9914\n",
      "Epoch 29/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9914\n",
      "Epoch 30/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9914\n",
      "Epoch 31/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.9913\n",
      "Epoch 32/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9913\n",
      "Epoch 33/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9913\n",
      "Epoch 34/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9912\n",
      "Epoch 35/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9912\n",
      "Epoch 36/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9912\n",
      "Epoch 37/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.9911\n",
      "Epoch 38/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.9911\n",
      "Epoch 39/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9911\n",
      "Epoch 40/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9910\n",
      "Epoch 41/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9910\n",
      "Epoch 42/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9910\n",
      "Epoch 43/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9909\n",
      "Epoch 44/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9909\n",
      "Epoch 45/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9909\n",
      "Epoch 46/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9908\n",
      "Epoch 47/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9908\n",
      "Epoch 48/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9907\n",
      "Epoch 49/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.9907\n",
      "Epoch 50/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.9907\n",
      "Epoch 51/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9906\n",
      "Epoch 52/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9906\n",
      "Epoch 53/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9905\n",
      "Epoch 54/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9905\n",
      "Epoch 55/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9904\n",
      "Epoch 56/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.9904\n",
      "Epoch 57/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.9904\n",
      "Epoch 58/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9903\n",
      "Epoch 59/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9903\n",
      "Epoch 60/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9902\n",
      "Epoch 61/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.9902\n",
      "Epoch 62/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9901\n",
      "Epoch 63/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9901\n",
      "Epoch 64/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9900\n",
      "Epoch 65/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9900\n",
      "Epoch 66/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9899\n",
      "Epoch 67/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9899\n",
      "Epoch 68/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9898\n",
      "Epoch 69/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9898\n",
      "Epoch 70/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9897\n",
      "Epoch 71/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9897\n",
      "Epoch 72/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9897\n",
      "Epoch 73/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9896\n",
      "Epoch 74/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9896\n",
      "Epoch 75/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9895\n",
      "Epoch 76/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9895\n",
      "Epoch 77/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9894\n",
      "Epoch 78/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9894\n",
      "Epoch 79/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9894\n",
      "Epoch 80/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9893\n",
      "Epoch 81/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9893\n",
      "Epoch 82/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9892\n",
      "Epoch 83/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9892\n",
      "Epoch 84/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9892\n",
      "Epoch 85/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9891\n",
      "Epoch 86/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9891\n",
      "Epoch 87/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9891\n",
      "Epoch 88/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9890\n",
      "Epoch 89/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9890\n",
      "Epoch 90/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9890\n",
      "Epoch 91/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9889\n",
      "Epoch 92/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9889\n",
      "Epoch 93/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9888\n",
      "Epoch 94/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9888\n",
      "Epoch 95/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9888\n",
      "Epoch 96/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9888\n",
      "Epoch 97/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9887\n",
      "Epoch 98/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9887\n",
      "Epoch 99/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.9887\n",
      "Epoch 100/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9886\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step \n",
      "Mean Squared Error: 0.9903711941557266\n",
      "R2 Score: 0.0012772200458910365\n"
     ]
    }
   ],
   "source": [
    "# lstm with keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(100, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=10, shuffle=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# accuracy\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>3853.290039</td>\n",
       "      <td>3878.459961</td>\n",
       "      <td>3794.330078</td>\n",
       "      <td>3824.139893</td>\n",
       "      <td>3824.139893</td>\n",
       "      <td>3959140000</td>\n",
       "      <td>-0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>3840.360107</td>\n",
       "      <td>3873.159912</td>\n",
       "      <td>3815.770020</td>\n",
       "      <td>3852.969971</td>\n",
       "      <td>3852.969971</td>\n",
       "      <td>4414080000</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3802.419922</td>\n",
       "      <td>3808.100098</td>\n",
       "      <td>3808.100098</td>\n",
       "      <td>3893450000</td>\n",
       "      <td>-0.011646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>3823.370117</td>\n",
       "      <td>3906.189941</td>\n",
       "      <td>3809.560059</td>\n",
       "      <td>3895.080078</td>\n",
       "      <td>3895.080078</td>\n",
       "      <td>3923560000</td>\n",
       "      <td>0.022841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>3910.820068</td>\n",
       "      <td>3950.570068</td>\n",
       "      <td>3890.419922</td>\n",
       "      <td>3892.090088</td>\n",
       "      <td>3892.090088</td>\n",
       "      <td>4311770000</td>\n",
       "      <td>-0.000768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         Open         High          Low        Close  \\\n",
       "5787  2023-01-03  3853.290039  3878.459961  3794.330078  3824.139893   \n",
       "5788  2023-01-04  3840.360107  3873.159912  3815.770020  3852.969971   \n",
       "5789  2023-01-05  3839.739990  3839.739990  3802.419922  3808.100098   \n",
       "5790  2023-01-06  3823.370117  3906.189941  3809.560059  3895.080078   \n",
       "5791  2023-01-09  3910.820068  3950.570068  3890.419922  3892.090088   \n",
       "\n",
       "        Adj Close      Volume  daily_return  \n",
       "5787  3824.139893  3959140000     -0.004001  \n",
       "5788  3852.969971  4414080000      0.007539  \n",
       "5789  3808.100098  3893450000     -0.011646  \n",
       "5790  3895.080078  3923560000      0.022841  \n",
       "5791  3892.090088  4311770000     -0.000768  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch daily S&P 500 data\n",
    "sp500_data = yf.download('^GSPC', start='2000-01-01', end='2024-01-01')\n",
    "\n",
    "# Calculate additional features for S&P 500 data\n",
    "sp500_data['daily_return'] = sp500_data['Close'].pct_change() #might need\n",
    "\n",
    "merged_data=sp500_data\n",
    "#convert the index to Date column\n",
    "merged_data.reset_index(inplace=True)\n",
    "# \n",
    "merged_data['Date'] = merged_data['Date'].astype(str)\n",
    "\n",
    "\n",
    "#get data from 2023-01-01 to 2024-01-01\n",
    "merged_data = merged_data.loc[merged_data['Date'] >= '2023-01-01']\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 175 rows\n",
      "Test data: 37 rows\n",
      "Validate data: 38 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split my data into train and test and validate\n",
    "#70% train, 15% test, 15% validate\n",
    "\n",
    "\n",
    "train_data, temp_data = train_test_split(merged_data, test_size=0.3, random_state=42)\n",
    "\n",
    "test_data, validate_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f\"Train data: {len(train_data)} rows\")\n",
    "print(f\"Test data: {len(test_data)} rows\")\n",
    "print(f\"Validate data: {len(validate_data)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x35e405550>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 24))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(merged_data.Date, merged_data.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data_std = scaler.fit_transform(np.array(train_data['Close']).reshape(-1, 1))#reshape the data to 2D array\n",
    "test_data_std = scaler.transform(np.array(test_data['Close']).reshape(-1, 1))\n",
    "validate_data_std = scaler.transform(np.array(validate_data['Close']).reshape(-1, 1))\n",
    "\n",
    "\n",
    "# \n",
    "def create_shifted_data(data, shift=1):\n",
    "    X = data[:-shift]\n",
    "    y = data[shift:]\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_shifted_data(train_data_std)\n",
    "validate_X, validate_y = create_shifted_data(validate_data_std)\n",
    "\n",
    "\n",
    "# LSTM\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "validate_X = validate_X.reshape((validate_X.shape[0], 1, validate_X.shape[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_batch_size': 32,  # \n",
    "    'epochs': 250,  # \n",
    "    'output_path': 'output/',  # \n",
    "    'learning_rate': 0.03,  # \n",
    "    'lr_decay_rate': 0.8,  # \n",
    "    'num_of_decay_per_epoch': 6  #  epoch \n",
    "}\n",
    "\n",
    "train_batch_size = params['train_batch_size']\n",
    "epochs = params['epochs']\n",
    "output_path = params['output_path']\n",
    "learning_rate = params['learning_rate']\n",
    "lr_decay_rate = params['lr_decay_rate']\n",
    "num_of_decay_per_epoch = params['num_of_decay_per_epoch']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- We create for sp500\n",
    "Then we use same features for  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianqianmeng/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> \n",
       "\n",
       " lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> \n",
       "\n",
       " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> \n",
       "\n",
       " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m66,560\u001b[0m \n",
       "\n",
       " lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m49,408\u001b[0m \n",
       "\n",
       " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_10 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                      \u001b[38;5;34m1,040\u001b[0m \n",
       "\n",
       " dense_11 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m17\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,025</span> (457.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,025\u001b[0m (457.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,025</span> (457.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,025\u001b[0m (457.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "import math\n",
    "\n",
    "# Assuming learning_rate, epochs, num_of_decay_per_epoch, and lr_decay_rate are already defined\n",
    "\n",
    "# Building the LSTM neural network\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(1, 1), units=128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Specifying the learning rate decay schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=math.floor(epochs / num_of_decay_per_epoch),\n",
    "    decay_rate=lr_decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=['mean_squared_error']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - loss: 0.8231 - mean_squared_error: 0.8231\n",
      "Epoch 2/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5125 - mean_squared_error: 0.5125\n",
      "Epoch 3/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1311 - mean_squared_error: 0.1311\n",
      "Epoch 4/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0537 - mean_squared_error: 0.0537\n",
      "Epoch 5/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 6/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 7/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 8/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 9/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 10/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 11/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 12/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 13/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 14/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 15/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 16/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 17/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 18/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 19/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 20/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 21/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 22/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 23/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 24/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 25/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 26/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 27/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 28/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 29/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 30/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 31/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 32/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 33/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 34/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 35/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 36/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 37/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 38/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 39/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 40/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 41/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 42/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 43/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 44/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 45/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 46/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 47/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 48/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 49/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 50/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 51/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 52/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 53/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 54/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 55/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 56/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 57/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 58/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 59/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 60/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 61/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 62/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 63/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 64/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 65/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 66/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 67/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 68/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 69/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 70/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 71/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 72/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 73/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 74/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 75/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 76/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Epoch 77/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 78/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 79/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 80/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 81/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 82/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 83/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 84/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 85/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 86/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 87/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 88/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 89/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 90/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Epoch 91/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 92/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 93/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 94/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 95/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 96/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 97/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 98/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 99/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 100/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 101/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 102/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 103/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Epoch 104/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 105/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 106/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Epoch 107/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 108/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 109/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 110/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
      "Epoch 111/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 112/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 113/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 114/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 115/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 116/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 117/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 118/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0031 - mean_squared_error: 0.0031\n",
      "Epoch 119/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 120/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 121/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 122/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 123/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 124/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 125/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - mean_squared_error: 0.0031\n",
      "Epoch 126/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 127/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - mean_squared_error: 0.0030\n",
      "Epoch 128/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 129/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 130/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 131/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 132/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 133/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 134/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 135/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 136/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Epoch 137/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 138/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 139/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 140/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 141/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 142/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 143/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 144/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 145/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 146/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 147/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - mean_squared_error: 0.0032\n",
      "Epoch 148/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 149/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 150/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 151/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 152/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 153/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 154/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 155/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 156/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 157/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 158/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 159/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 160/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
      "Epoch 161/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 162/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 163/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 164/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 165/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 166/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 167/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 168/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 169/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 170/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 171/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 172/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 173/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 174/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 175/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 176/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 177/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 178/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 179/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 180/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 181/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 182/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "Epoch 183/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032 - mean_squared_error: 0.0032\n",
      "Epoch 184/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Epoch 185/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 186/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 187/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n",
      "Epoch 188/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 189/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 190/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0031 - mean_squared_error: 0.0031\n",
      "Epoch 191/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 192/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 193/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 194/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 195/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 196/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 197/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 198/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Epoch 199/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
      "Epoch 200/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 201/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Epoch 202/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 203/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "Epoch 204/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "Epoch 205/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 206/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 207/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 208/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 209/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 210/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 211/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 212/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 213/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 214/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 215/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0041 - mean_squared_error: 0.0041\n",
      "Epoch 216/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 217/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 218/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 219/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 220/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 221/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 222/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 223/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 224/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n",
      "Epoch 225/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 226/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 227/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 228/250\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model (assuming training data is available)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m validate_data_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(validate_data_std)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model (assuming training data is available)\n",
    "model.fit(train_data_std, train_data_std, epochs=epochs, batch_size=32)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "validate_data_pred = model.predict(validate_data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4357.184643347862\n",
      "MAE: 4352.473239759767\n",
      "MAPE: 99.99226186007863\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "validate_data_Y = np.array(validate_data['Close']).reshape(-1, 1)\n",
    "\n",
    "# Assuming validate_data_pred and validate_data_Y are already defined\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "RMSE = np.sqrt(np.mean((validate_data_pred - validate_data_Y) ** 2))\n",
    "\n",
    "# Calculate MAE (Mean Absolute Error)\n",
    "MAE = np.mean(np.abs(validate_data_pred - validate_data_Y))\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "MAPE = np.mean(np.abs((validate_data_pred - validate_data_Y) / validate_data_Y)) * 100\n",
    "\n",
    "# Print the error metrics\n",
    "print(\"RMSE:\", RMSE)\n",
    "print(\"MAE:\", MAE)\n",
    "print(\"MAPE:\", MAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianqianmeng/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - loss: 1.0370 - mean_squared_error: 1.0370\n",
      "Epoch 2/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9845 - mean_squared_error: 0.9845\n",
      "Epoch 3/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9879 - mean_squared_error: 0.9879\n",
      "Epoch 4/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0180 - mean_squared_error: 1.0180\n",
      "Epoch 5/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9937 - mean_squared_error: 0.9937\n",
      "Epoch 6/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9871 - mean_squared_error: 0.9871\n",
      "Epoch 7/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0124 - mean_squared_error: 1.0124\n",
      "Epoch 8/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9868 - mean_squared_error: 0.9868\n",
      "Epoch 9/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9993 - mean_squared_error: 0.9993\n",
      "Epoch 10/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9967 - mean_squared_error: 0.9967\n",
      "Epoch 11/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0024 - mean_squared_error: 1.0024\n",
      "Epoch 12/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0026 - mean_squared_error: 1.0026\n",
      "Epoch 13/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9962 - mean_squared_error: 0.9962\n",
      "Epoch 14/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9926 - mean_squared_error: 0.9926\n",
      "Epoch 15/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0257 - mean_squared_error: 1.0257\n",
      "Epoch 16/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0132 - mean_squared_error: 1.0132\n",
      "Epoch 17/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9993 - mean_squared_error: 0.9993\n",
      "Epoch 18/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9813 - mean_squared_error: 0.9813\n",
      "Epoch 19/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9840 - mean_squared_error: 0.9840\n",
      "Epoch 20/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9994 - mean_squared_error: 0.9994\n",
      "Epoch 21/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9750 - mean_squared_error: 0.9750\n",
      "Epoch 22/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9625 - mean_squared_error: 0.9625\n",
      "Epoch 23/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0287 - mean_squared_error: 1.0287\n",
      "Epoch 24/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0045 - mean_squared_error: 1.0045\n",
      "Epoch 25/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9851 - mean_squared_error: 0.9851\n",
      "Epoch 26/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0001 - mean_squared_error: 1.0001\n",
      "Epoch 27/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.9874 - mean_squared_error: 0.9874\n",
      "Epoch 28/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 1.0356 - mean_squared_error: 1.0356\n",
      "Epoch 29/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0335 - mean_squared_error: 1.0335\n",
      "Epoch 30/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9728 - mean_squared_error: 0.9728\n",
      "Epoch 31/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 1.0241 - mean_squared_error: 1.0241\n",
      "Epoch 32/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9846 - mean_squared_error: 0.9846\n",
      "Epoch 33/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0030 - mean_squared_error: 1.0030\n",
      "Epoch 34/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 1.0304 - mean_squared_error: 1.0304\n",
      "Epoch 35/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.9891 - mean_squared_error: 0.9891\n",
      "Epoch 36/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 1.0420 - mean_squared_error: 1.0420\n",
      "Epoch 37/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9873 - mean_squared_error: 0.9873\n",
      "Epoch 38/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9958 - mean_squared_error: 0.9958\n",
      "Epoch 39/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9763 - mean_squared_error: 0.9763\n",
      "Epoch 40/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0265 - mean_squared_error: 1.0265\n",
      "Epoch 41/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0151 - mean_squared_error: 1.0151\n",
      "Epoch 42/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0214 - mean_squared_error: 1.0214\n",
      "Epoch 43/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9856 - mean_squared_error: 0.9856\n",
      "Epoch 44/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0002 - mean_squared_error: 1.0002\n",
      "Epoch 45/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9554 - mean_squared_error: 0.9554\n",
      "Epoch 46/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0200 - mean_squared_error: 1.0200\n",
      "Epoch 47/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0077 - mean_squared_error: 1.0077\n",
      "Epoch 48/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9987 - mean_squared_error: 0.9987\n",
      "Epoch 49/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9938 - mean_squared_error: 0.9938\n",
      "Epoch 50/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9982 - mean_squared_error: 0.9982\n",
      "Epoch 51/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9877 - mean_squared_error: 0.9877\n",
      "Epoch 52/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9904 - mean_squared_error: 0.9904\n",
      "Epoch 53/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9962 - mean_squared_error: 0.9962\n",
      "Epoch 54/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0122 - mean_squared_error: 1.0122\n",
      "Epoch 55/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9975 - mean_squared_error: 0.9975\n",
      "Epoch 56/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9704 - mean_squared_error: 0.9704\n",
      "Epoch 57/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9939 - mean_squared_error: 0.9939\n",
      "Epoch 58/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0261 - mean_squared_error: 1.0261\n",
      "Epoch 59/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9933 - mean_squared_error: 0.9933\n",
      "Epoch 60/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0366 - mean_squared_error: 1.0366\n",
      "Epoch 61/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0005 - mean_squared_error: 1.0005\n",
      "Epoch 62/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.9766 - mean_squared_error: 0.9766\n",
      "Epoch 63/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0454 - mean_squared_error: 1.0454\n",
      "Epoch 64/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.9801 - mean_squared_error: 0.9801\n",
      "Epoch 65/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 1.0028 - mean_squared_error: 1.0028\n",
      "Epoch 66/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9978 - mean_squared_error: 0.9978\n",
      "Epoch 67/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0446 - mean_squared_error: 1.0446\n",
      "Epoch 68/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9803 - mean_squared_error: 0.9803\n",
      "Epoch 69/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9856 - mean_squared_error: 0.9856\n",
      "Epoch 70/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0452 - mean_squared_error: 1.0452\n",
      "Epoch 71/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.9939 - mean_squared_error: 0.9939\n",
      "Epoch 72/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.9961 - mean_squared_error: 0.9961\n",
      "Epoch 73/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.9969 - mean_squared_error: 0.9969\n",
      "Epoch 74/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9997 - mean_squared_error: 0.9997\n",
      "Epoch 75/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9864 - mean_squared_error: 0.9864\n",
      "Epoch 76/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9986 - mean_squared_error: 0.9986\n",
      "Epoch 77/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0158 - mean_squared_error: 1.0158\n",
      "Epoch 78/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9919 - mean_squared_error: 0.9919\n",
      "Epoch 79/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9879 - mean_squared_error: 0.9879\n",
      "Epoch 80/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0000 - mean_squared_error: 1.0000\n",
      "Epoch 81/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9996 - mean_squared_error: 0.9996\n",
      "Epoch 82/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9874 - mean_squared_error: 0.9874\n",
      "Epoch 83/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0031 - mean_squared_error: 1.0031\n",
      "Epoch 84/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9727 - mean_squared_error: 0.9727\n",
      "Epoch 85/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9970 - mean_squared_error: 0.9970\n",
      "Epoch 86/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9829 - mean_squared_error: 0.9829\n",
      "Epoch 87/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9970 - mean_squared_error: 0.9970\n",
      "Epoch 88/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0175 - mean_squared_error: 1.0175\n",
      "Epoch 89/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0231 - mean_squared_error: 1.0231\n",
      "Epoch 90/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0174 - mean_squared_error: 1.0174\n",
      "Epoch 91/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9985 - mean_squared_error: 0.9985\n",
      "Epoch 92/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 1.0164 - mean_squared_error: 1.0164\n",
      "Epoch 93/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9956 - mean_squared_error: 0.9956\n",
      "Epoch 94/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0245 - mean_squared_error: 1.0245\n",
      "Epoch 95/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9650 - mean_squared_error: 0.9650\n",
      "Epoch 96/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9967 - mean_squared_error: 0.9967\n",
      "Epoch 97/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0175 - mean_squared_error: 1.0175\n",
      "Epoch 98/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0026 - mean_squared_error: 1.0026\n",
      "Epoch 99/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9761 - mean_squared_error: 0.9761\n",
      "Epoch 100/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0093 - mean_squared_error: 1.0093\n",
      "Epoch 101/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0051 - mean_squared_error: 1.0051\n",
      "Epoch 102/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0133 - mean_squared_error: 1.0133\n",
      "Epoch 103/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0392 - mean_squared_error: 1.0392\n",
      "Epoch 104/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9620 - mean_squared_error: 0.9620\n",
      "Epoch 105/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9915 - mean_squared_error: 0.9915\n",
      "Epoch 106/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9966 - mean_squared_error: 0.9966\n",
      "Epoch 107/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9653 - mean_squared_error: 0.9653\n",
      "Epoch 108/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0085 - mean_squared_error: 1.0085\n",
      "Epoch 109/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0237 - mean_squared_error: 1.0237\n",
      "Epoch 110/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9944 - mean_squared_error: 0.9944\n",
      "Epoch 111/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0028 - mean_squared_error: 1.0028\n",
      "Epoch 112/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0009 - mean_squared_error: 1.0009\n",
      "Epoch 113/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0080 - mean_squared_error: 1.0080\n",
      "Epoch 114/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9824 - mean_squared_error: 0.9824\n",
      "Epoch 115/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0088 - mean_squared_error: 1.0088\n",
      "Epoch 116/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0138 - mean_squared_error: 1.0138\n",
      "Epoch 117/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0183 - mean_squared_error: 1.0183\n",
      "Epoch 118/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9892 - mean_squared_error: 0.9892\n",
      "Epoch 119/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0041 - mean_squared_error: 1.0041\n",
      "Epoch 120/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9900 - mean_squared_error: 0.9900\n",
      "Epoch 121/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0281 - mean_squared_error: 1.0281\n",
      "Epoch 122/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0233 - mean_squared_error: 1.0233\n",
      "Epoch 123/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9952 - mean_squared_error: 0.9952\n",
      "Epoch 124/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9898 - mean_squared_error: 0.9898\n",
      "Epoch 125/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0256 - mean_squared_error: 1.0256\n",
      "Epoch 126/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9868 - mean_squared_error: 0.9868\n",
      "Epoch 127/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9737 - mean_squared_error: 0.9737\n",
      "Epoch 128/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0095 - mean_squared_error: 1.0095\n",
      "Epoch 129/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9991 - mean_squared_error: 0.9991\n",
      "Epoch 130/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9719 - mean_squared_error: 0.9719\n",
      "Epoch 131/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9889 - mean_squared_error: 0.9889\n",
      "Epoch 132/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9898 - mean_squared_error: 0.9898\n",
      "Epoch 133/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9985 - mean_squared_error: 0.9985\n",
      "Epoch 134/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0162 - mean_squared_error: 1.0162\n",
      "Epoch 135/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0424 - mean_squared_error: 1.0424\n",
      "Epoch 136/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9895 - mean_squared_error: 0.9895\n",
      "Epoch 137/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9817 - mean_squared_error: 0.9817\n",
      "Epoch 138/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9598 - mean_squared_error: 0.9598\n",
      "Epoch 139/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9286 - mean_squared_error: 0.9286\n",
      "Epoch 140/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9771 - mean_squared_error: 0.9771\n",
      "Epoch 141/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9777 - mean_squared_error: 0.9777\n",
      "Epoch 142/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9990 - mean_squared_error: 0.9990\n",
      "Epoch 143/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0194 - mean_squared_error: 1.0194\n",
      "Epoch 144/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0056 - mean_squared_error: 1.0056\n",
      "Epoch 145/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9990 - mean_squared_error: 0.9990\n",
      "Epoch 146/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0026 - mean_squared_error: 1.0026\n",
      "Epoch 147/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9777 - mean_squared_error: 0.9777\n",
      "Epoch 148/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0193 - mean_squared_error: 1.0193\n",
      "Epoch 149/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9811 - mean_squared_error: 0.9811\n",
      "Epoch 150/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9663 - mean_squared_error: 0.9663\n",
      "Epoch 151/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9944 - mean_squared_error: 0.9944\n",
      "Epoch 152/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9453 - mean_squared_error: 0.9453\n",
      "Epoch 153/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9944 - mean_squared_error: 0.9944\n",
      "Epoch 154/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0053 - mean_squared_error: 1.0053\n",
      "Epoch 155/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0131 - mean_squared_error: 1.0131\n",
      "Epoch 156/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9716 - mean_squared_error: 0.9716\n",
      "Epoch 157/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0289 - mean_squared_error: 1.0289\n",
      "Epoch 158/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9718 - mean_squared_error: 0.9718\n",
      "Epoch 159/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9864 - mean_squared_error: 0.9864\n",
      "Epoch 160/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9985 - mean_squared_error: 0.9985\n",
      "Epoch 161/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0568 - mean_squared_error: 1.0568\n",
      "Epoch 162/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0371 - mean_squared_error: 1.0371\n",
      "Epoch 163/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9917 - mean_squared_error: 0.9917\n",
      "Epoch 164/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0141 - mean_squared_error: 1.0141\n",
      "Epoch 165/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9918 - mean_squared_error: 0.9918\n",
      "Epoch 166/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0235 - mean_squared_error: 1.0235\n",
      "Epoch 167/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0275 - mean_squared_error: 1.0275\n",
      "Epoch 168/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9866 - mean_squared_error: 0.9866\n",
      "Epoch 169/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0397 - mean_squared_error: 1.0397\n",
      "Epoch 170/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9890 - mean_squared_error: 0.9890\n",
      "Epoch 171/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0111 - mean_squared_error: 1.0111\n",
      "Epoch 172/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9772 - mean_squared_error: 0.9772\n",
      "Epoch 173/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0204 - mean_squared_error: 1.0204\n",
      "Epoch 174/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9725 - mean_squared_error: 0.9725\n",
      "Epoch 175/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9851 - mean_squared_error: 0.9851\n",
      "Epoch 176/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9658 - mean_squared_error: 0.9658\n",
      "Epoch 177/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9979 - mean_squared_error: 0.9979\n",
      "Epoch 178/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9956 - mean_squared_error: 0.9956\n",
      "Epoch 179/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0182 - mean_squared_error: 1.0182\n",
      "Epoch 180/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9686 - mean_squared_error: 0.9686\n",
      "Epoch 181/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9924 - mean_squared_error: 0.9924\n",
      "Epoch 182/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9472 - mean_squared_error: 0.9472\n",
      "Epoch 183/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0595 - mean_squared_error: 1.0595\n",
      "Epoch 184/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0122 - mean_squared_error: 1.0122\n",
      "Epoch 185/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9968 - mean_squared_error: 0.9968\n",
      "Epoch 186/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0344 - mean_squared_error: 1.0344\n",
      "Epoch 187/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0035 - mean_squared_error: 1.0035\n",
      "Epoch 188/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9843 - mean_squared_error: 0.9843\n",
      "Epoch 189/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9951 - mean_squared_error: 0.9951\n",
      "Epoch 190/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0062 - mean_squared_error: 1.0062\n",
      "Epoch 191/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9920 - mean_squared_error: 0.9920\n",
      "Epoch 192/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0411 - mean_squared_error: 1.0411\n",
      "Epoch 193/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9836 - mean_squared_error: 0.9836\n",
      "Epoch 194/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0275 - mean_squared_error: 1.0275\n",
      "Epoch 195/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0337 - mean_squared_error: 1.0337\n",
      "Epoch 196/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0079 - mean_squared_error: 1.0079\n",
      "Epoch 197/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0281 - mean_squared_error: 1.0281\n",
      "Epoch 198/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9960 - mean_squared_error: 0.9960\n",
      "Epoch 199/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0263 - mean_squared_error: 1.0263\n",
      "Epoch 200/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9852 - mean_squared_error: 0.9852\n",
      "Epoch 201/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0206 - mean_squared_error: 1.0206\n",
      "Epoch 202/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9964 - mean_squared_error: 0.9964\n",
      "Epoch 203/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.9888 - mean_squared_error: 0.9888\n",
      "Epoch 204/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0179 - mean_squared_error: 1.0179\n",
      "Epoch 205/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0041 - mean_squared_error: 1.0041\n",
      "Epoch 206/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0346 - mean_squared_error: 1.0346\n",
      "Epoch 207/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9780 - mean_squared_error: 0.9780\n",
      "Epoch 208/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9963 - mean_squared_error: 0.9963\n",
      "Epoch 209/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9912 - mean_squared_error: 0.9912\n",
      "Epoch 210/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9959 - mean_squared_error: 0.9959\n",
      "Epoch 211/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0362 - mean_squared_error: 1.0362\n",
      "Epoch 212/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9995 - mean_squared_error: 0.9995\n",
      "Epoch 213/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.9754 - mean_squared_error: 0.9754\n",
      "Epoch 214/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.9840 - mean_squared_error: 0.9840\n",
      "Epoch 215/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 1.0156 - mean_squared_error: 1.0156\n",
      "Epoch 216/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.9777 - mean_squared_error: 0.9777\n",
      "Epoch 217/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.9922 - mean_squared_error: 0.9922\n",
      "Epoch 218/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 1.0485 - mean_squared_error: 1.0485\n",
      "Epoch 219/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.9706 - mean_squared_error: 0.9706\n",
      "Epoch 220/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 1.0039 - mean_squared_error: 1.0039\n",
      "Epoch 221/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 1.0113 - mean_squared_error: 1.0113\n",
      "Epoch 222/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.9832 - mean_squared_error: 0.9832\n",
      "Epoch 223/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0023 - mean_squared_error: 1.0023\n",
      "Epoch 224/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9816 - mean_squared_error: 0.9816\n",
      "Epoch 225/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0453 - mean_squared_error: 1.0453\n",
      "Epoch 226/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0137 - mean_squared_error: 1.0137\n",
      "Epoch 227/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9690 - mean_squared_error: 0.9690\n",
      "Epoch 228/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9788 - mean_squared_error: 0.9788\n",
      "Epoch 229/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0107 - mean_squared_error: 1.0107\n",
      "Epoch 230/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9983 - mean_squared_error: 0.9983\n",
      "Epoch 231/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.9720 - mean_squared_error: 0.9720\n",
      "Epoch 232/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0318 - mean_squared_error: 1.0318\n",
      "Epoch 233/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 1.0149 - mean_squared_error: 1.0149\n",
      "Epoch 234/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 1.0105 - mean_squared_error: 1.0105\n",
      "Epoch 235/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.9796 - mean_squared_error: 0.9796\n",
      "Epoch 236/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.9848 - mean_squared_error: 0.9848\n",
      "Epoch 237/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.9900 - mean_squared_error: 0.9900\n",
      "Epoch 238/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 1.0020 - mean_squared_error: 1.0020\n",
      "Epoch 239/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0109 - mean_squared_error: 1.0109\n",
      "Epoch 240/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0168 - mean_squared_error: 1.0168\n",
      "Epoch 241/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.9780 - mean_squared_error: 0.9780\n",
      "Epoch 242/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 1.0017 - mean_squared_error: 1.0017\n",
      "Epoch 243/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 1.0121 - mean_squared_error: 1.0121\n",
      "Epoch 244/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0124 - mean_squared_error: 1.0124\n",
      "Epoch 245/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9800 - mean_squared_error: 0.9800\n",
      "Epoch 246/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.9856 - mean_squared_error: 0.9856\n",
      "Epoch 247/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.9978 - mean_squared_error: 0.9978\n",
      "Epoch 248/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 1.0049 - mean_squared_error: 1.0049\n",
      "Epoch 249/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.9848 - mean_squared_error: 0.9848\n",
      "Epoch 250/250\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 1.0269 - mean_squared_error: 1.0269\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step\n",
      "RMSE: 1062.075534958708\n",
      "MAE: 878.8861223493303\n",
      "MAPE: 51.51289596808862\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "train_data, temp_data = train_test_split(merged_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# \n",
    "test_data, validate_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# \n",
    "scaler = StandardScaler()\n",
    "train_data_std = scaler.fit_transform(np.array(train_data['Close']).reshape(-1, 1))\n",
    "test_data_std = scaler.transform(np.array(test_data['Close']).reshape(-1, 1))\n",
    "validate_data_std = scaler.transform(np.array(validate_data['Close']).reshape(-1, 1))\n",
    "\n",
    "# \n",
    "def create_shifted_data(data, shift=1):\n",
    "    X = data[:-shift]\n",
    "    y = data[shift:]\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_shifted_data(train_data_std)\n",
    "validate_X, validate_y = create_shifted_data(validate_data_std)\n",
    "\n",
    "# LSTM\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "validate_X = validate_X.reshape((validate_X.shape[0], 1, validate_X.shape[1]))\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(1, train_X.shape[2]), units=128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# \n",
    "learning_rate = 0.03\n",
    "epochs = 250\n",
    "num_of_decay_per_epoch = 6\n",
    "lr_decay_rate = 0.8\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=math.floor(epochs / num_of_decay_per_epoch),\n",
    "    decay_rate=lr_decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=['mean_squared_error']\n",
    ")\n",
    "\n",
    "# \n",
    "model.fit(train_X, train_y, epochs=epochs, batch_size=32)\n",
    "\n",
    "# \n",
    "validate_data_pred = model.predict(validate_X)\n",
    "\n",
    "# \n",
    "validate_data_pred_inv = scaler.inverse_transform(validate_data_pred)\n",
    "validate_data_y_inv = scaler.inverse_transform(validate_y)\n",
    "\n",
    "# \n",
    "RMSE = np.sqrt(np.mean((validate_data_pred_inv - validate_data_y_inv) ** 2))\n",
    "MAE = np.mean(np.abs(validate_data_pred_inv - validate_data_y_inv))\n",
    "MAPE = np.mean(np.abs((validate_data_pred_inv - validate_data_y_inv) / validate_data_y_inv)) * 100\n",
    "\n",
    "# \n",
    "print(\"RMSE:\", RMSE)\n",
    "print(\"MAE:\", MAE)\n",
    "print(\"MAPE:\", MAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tried: Data with GPD and unemployment rate, inflation.....make the model more inaccurate, bigger RMSE, MAE, MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianqianmeng/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 129ms/step - loss: 1.0336 - mean_squared_error: 1.0336 - val_loss: 0.9651 - val_mean_squared_error: 0.9651\n",
      "Epoch 2/500\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - loss: 0.9243 - mean_squared_error: 0.9243 - val_loss: 0.9668 - val_mean_squared_error: 0.9668\n",
      "Epoch 3/500\n",
      "\u001b[1m 81/128\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 1.0186 - mean_squared_error: 1.0186"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 65\u001b[0m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     59\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     60\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[1;32m     61\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidate_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     68\u001b[0m validate_data_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(validate_X)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "train_data, temp_data = train_test_split(merged_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# \n",
    "test_data, validate_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# \n",
    "scaler = StandardScaler()\n",
    "train_data_std = scaler.fit_transform(np.array(train_data['Close']).reshape(-1, 1))\n",
    "test_data_std = scaler.transform(np.array(test_data['Close']).reshape(-1, 1))\n",
    "validate_data_std = scaler.transform(np.array(validate_data['Close']).reshape(-1, 1))\n",
    "\n",
    "# \n",
    "def create_shifted_data(data, shift=1):\n",
    "    X = data[:-shift]\n",
    "    y = data[shift:]\n",
    "    return X, y\n",
    "\n",
    "train_X, train_y = create_shifted_data(train_data_std)\n",
    "validate_X, validate_y = create_shifted_data(validate_data_std)\n",
    "\n",
    "# LSTM\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "validate_X = validate_X.reshape((validate_X.shape[0], 1, validate_X.shape[1]))\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(input_shape=(1, train_X.shape[2]), units=128, return_sequences=True)))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='linear'))\n",
    "model.add(Dense(16, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# \n",
    "learning_rate = 0.03\n",
    "epochs = 500\n",
    "num_of_decay_per_epoch = 6\n",
    "lr_decay_rate = 0.8\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=math.floor(epochs / num_of_decay_per_epoch),\n",
    "    decay_rate=lr_decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=['mean_squared_error']\n",
    ")\n",
    "\n",
    "# \n",
    "model.fit(train_X, train_y, epochs=epochs, batch_size=32, validation_data=(validate_X, validate_y), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "# \n",
    "validate_data_pred = model.predict(validate_X)\n",
    "\n",
    "# \n",
    "validate_data_pred_inv = scaler.inverse_transform(validate_data_pred)\n",
    "validate_data_y_inv = scaler.inverse_transform(validate_y)\n",
    "\n",
    "# \n",
    "RMSE = np.sqrt(np.mean((validate_data_pred_inv - validate_data_y_inv) ** 2))\n",
    "MAE = np.mean(np.abs(validate_data_pred_inv - validate_data_y_inv))\n",
    "MAPE = np.mean(np.abs((validate_data_pred_inv - validate_data_y_inv) / validate_data_y_inv)) * 100\n",
    "\n",
    "# \n",
    "print(\"RMSE:\", RMSE)\n",
    "print(\"MAE:\", MAE)\n",
    "print(\"MAPE:\", MAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "differ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
